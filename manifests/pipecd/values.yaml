# Service and ingress.
service:
  port: 8080
  internalPort:
  type: "NodePort"

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.allow-http: "false"
    # kubernetes.io/ingress.global-static-ip-name: pipecd

# Workloads.
gateway:
  replicasCount: 1
  imageTag: v1.10.0
  resources: {}
  internalTLS:
    enabled: false

server:
  replicasCount: 1
  args:
    cacheAddress: ""
    enableGRPCReflection: false
    secureCookie: false
    logEncoding: humanize
  resources: {}
  env: []

cache:
  imageTag: "5.0.5-alpine3.9"
  password: ""
  resources: {}

ops:
  args:
    logEncoding: humanize
  resources: {}

mysql:
  imageTag: "8.0.23"
  resources: {}

minio:
  imageTag: "RELEASE.2020-08-26T00-00-49Z"
  resources: {}

# Control Plane Configurations.
config:
  # Specifies whether a ConfigMap for control plane configuration should be created
  create: true
  # The name of the ConfigMap to use when create is false.
  name: ""
  # The name of the configuration file.
  fileName: control-plane-config.yaml
  # Configuration data for control plane when create is false.
  data: |
    apiVersion: "pipecd.dev/v1beta1"
    kind: ControlPlane
    spec:

# Secret files that can be used in above configuration.
secret:
  # Specifies whether a Secret for storing sensitive data should be created.
  create: true
  # The name of the Secret should be mounted to container.
  name: "pipecd-secrets"
  mountPath: /etc/pipecd-secret
  encryptionKey:
    fileName: "encryption-key"
    data: ""
  firestoreServiceAccount:
    fileName: "firestore-service-account"
    data: ""
  gcsServiceAccount:
    fileName: "gcs-service-account"
    data: ""
  minioAccessKey:
    fileName: "minio-access-key"
    data: ""
  minioSecretKey:
    fileName: "minio-secret-key"
    data: ""
  internalTLSKey:
    fileName: "internal-tls.key"
    data: ""
  internalTLSCert:
    fileName: "internal-tls.cert"
    data: ""


# Optional configuration for GKE.
backendConfig:
  enabled: false
  iap:
    enabled: false
    secretName: pipecd-iap

managedCertificate:
  enabled: false
  domains: []

cors:
  enabled: false
  allowOrigins:
    - "http://localhost:9090"

quickstart: false

monitoring:
  # If true, prometheus and grafana sub-charts will be installed
  enabled: false

# All directives inside this section will be directly sent to the prometheus chart.
# Head to the below link to see all available values.
# https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml
prometheus:
  pushgateway:
    enabled: false
  nodeExporter:
    enabled: false
  kubeStateMetrics:
    enabled: false
  configmapReload:
    prometheus:
      enabled: true
  alertmanager:
    enabled: true
    useClusterRole: true
  alertmanagerFiles:
    alertmanager.yml:
      # The root node of the routing tree.
      route: {}
      # A list of notification receivers.
      receivers: []
  server:
    # Must be fixed so that Grafana can find out statically.
    fullnameOverride: pipecd-prometheus-server
  serverFiles:
    prometheus.yml:
      rule_files:
        - /etc/config/recording_rules.yml
        - /etc/config/alerting_rules.yml

      scrape_configs:
        - job_name: prometheus
          static_configs:
            - targets:
                - localhost:9090
        - job_name: pipecd-gateway
          scrape_interval: 1m
          metrics_path: /stats/prometheus
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
              action: keep
              regex: pipecd
            - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_component]
              action: keep
              regex: gateway
            - source_labels: [__meta_kubernetes_pod_container_port_name]
              action: keep
              regex: envoy-admin
        - job_name: pipecd-server
          scrape_interval: 1m
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
              action: keep
              regex: pipecd
            - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_component]
              action: keep
              regex: server
            - source_labels: [__meta_kubernetes_pod_container_port_name]
              action: keep
              regex: admin
        - job_name: pipecd-ops
          scrape_interval: 1m
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
              action: keep
              regex: pipecd
            - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_component]
              action: keep
              regex: ops
            - source_labels: [__meta_kubernetes_pod_container_port_name]
              action: keep
              regex: admin

    # FIXME: Add rules
    alerting_rules.yml:
      groups:
       - name: Instances
         rules:
           - alert: InstanceDown
             expr: up == 0
             for: 5m
             labels:
               severity: page
             annotations:
               description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
               summary: 'Instance {{ $labels.instance }} down'
# All directives inside this section will be directly sent to the grafana chart.
# Head to the below link to see all available values.
# https://github.com/grafana/helm-charts/tree/main/charts/grafana
grafana:
  adminPassword: admin
  sidecar:
    datasources:
      enabled: true
    dashboards:
      enabled: true
      # Label that the configmaps with dashboards are marked with
      label: grafana_dashboard
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://pipecd-prometheus-server
          access: proxy
          version: 1
